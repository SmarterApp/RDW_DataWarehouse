###
# udl configuration
###

common:
  '[app]':
    celery:
        root: udl2.celery
        broker: amqp://guest@localhost//    # this is the url to message broker. Currently it is located on localhost for rabbitmq
        backend: amqp://guest@localhost//   # this is the url to task results for each request. Currently it is located on localhost for rabbitmq
        include:
            - edudl2.udl2.W_schedule_pipeline
            - edudl2.udl2.W_file_arrived
            - edudl2.udl2.W_file_decrypter
            - edudl2.udl2.W_file_expander
            - edudl2.udl2.W_get_params
            - edudl2.udl2.W_simple_file_validator
            - edudl2.udl2.W_file_splitter
            - edudl2.udl2.W_parallel_csv_load
            - edudl2.udl2.W_load_csv_to_staging
            - edudl2.udl2.W_file_content_validator
            - edudl2.udl2.W_load_json_to_integration
            - edudl2.udl2.W_load_to_integration_table
            - edudl2.udl2.W_determine_end_chain
            - edudl2.udl2.W_load_sr_integration_to_target
            - edudl2.udl2.W_load_from_integration_to_star
            - edudl2.udl2.W_post_etl
            - edudl2.udl2.W_report_error_or_success
            - edudl2.udl2.W_all_done
            - edudl2.udl2.W_tasks_utils
            - edudl2.benchmarking.run_benchmarks
    celery_defaults:
        CELERY_DEFAULT_QUEUE: celery        # default celery queue name for celery internal tasks
        CELERY_DEFAULT_EXCHANGE: direct     # default celery exchange name, and exchange type
        CELERY_DEFAULT_ROUTING_KEY: celery  # default celery routing key for tasks
        CELERYD_CONCURRENCY: 4              # number of available workers processes
    rabbitmq:                               # rabbitmq server for local testing if we requires to bring up a rabbitmq server for UDL2 celery tasks on this machine. It will be ignore by celery if there are global rabbitmq-server
        RABBITMQ_SERVER_PATH:
            - /opt/local/sbin/rabbitmq-server
            - /usr/local/sbin/rabbitmq-server       # where the rabbitmq-server is located, we list all possible locations in your system.
    file_splitter:                          # Options for file_splitter
        row_limit: 10000                    # default row number for split files
        parts: 1                            # default parts of files
        output_path: .                      # where the newly generated split file located
        keep_headers: True                  # preserve csv header for importing
    quiet_mode: False
    passphrase: sbac udl2
    db_defaults:
      echo: False
      max_overflow: 0
      pool_size: 2
    student_reg_guid_key:
        studentregistration: identification.guid
    reg_system_id_key:
        studentregistration: source.testregsysid
    callback_url_key:
        studentregistration: source.testregcallbackurl
    academic_year_key:
        studentregistration: identification.academicyear
    sr_notification_max_attempts: 5
    sr_notification_retry_interval: 30
    sr_notification_timeout_interval: 30
    logging:        # log location. this should be in the long run as file locations or more sophisticated logging system
        level: INFO
        debug: FALSE
        audit: /opt/edware/log/udl2.audit.log  # for status log for everything
        error: /opt/edware/log/udl2.error.log  # for error message and exceptions
    udl2_trigger:
        base_dir: /opt/edware/zones/landing
        source_dir: arrivals
        file_patterns_to_watch:
            - '*.gpg'
            - '*.gpg.done'
        file_stat_watch_interval: 2
        file_stat_watch_period: 10
        file_checksum_threshold_wait_period: 30
        file_system_scan_delay: 10
        
qa:
  '[app]':
    zones:                                              # zones for where the files are uploaded and processed. it may change to other mechanisms, but we uses local file system for the moment.
        landing: /opt/edware/zones/landing/             # this is for where the uploaded files are located, it may be an url in the long run to get data
        arrivals: /opt/edware/zones/landing/arrivals/   # this is where the file arrives.
        work: /opt/edware/zones/landing/work/           # this is the where the file are use for work. this should always be local for speed
        history: /opt/edware/zones/landing/history/     # this is where we store historical info. it may be an url for large file storages such as s3.
    rsync:
      command: rsync -az --exclude "*.partial" --remove-source-files {remote_user}@{remote_host}:{remote_dir} {landing}
      enable: True
      schedule:
          cron:
            hour: '*/1'
      args:
        remote_user: udl2
        remote_host: lz.foo.com
        remote_dir: /sftp/opt/edware/staging/
        landing: /opt/edware/zones/landing/
    udl2_trigger:
        file_stat_watch_interval: 10
        file_stat_watch_period: 120
        file_checksum_threshold_wait_period: 60
        file_system_scan_delay: 60
    udl2_db_conn:
        url: postgresql://udl2:udl2abc1234@dbpgudl0.qa.dum.edwdc.net:5432/udl2
        db_schema: udl2
    target_db_conn:
        ca:
            url: postgresql://edware:edware2013@dbpgudl0.qa.dum.edwdc.net:5432/edware
        cat:
            url: postgresql://edware:edware2013@dbpgudl0.qa.dum.edwdc.net:5432/edware
    prod_db_conn:
        ca:
            url: postgresql://edware:edware2013@dbpgdw0.qa.dum.edwdc.net:5432/edware
            db_schema: edware_es_1_7
        cat:
            url: postgresql://edware:edware2013@dbpgdw0.qa.dum.edwdc.net:5432/edware
            db_schema: edware_es_1_7
    gpg_home: /opt/edware/keys
    edware_stats:
        db:
            schema_name: edware_es_1_7
            url: postgresql://edware:edware2013@dbpgdw0.qa.dum.edwdc.net:5432/edware_stats

uat:
  '[app]':
    zones:                                              # zones for where the files are uploaded and processed. it may change to other mechanisms, but we uses local file system for the moment.
        landing: /opt/edware/zones/landing/             # this is for where the uploaded files are located, it may be an url in the long run to get data
        arrivals: /opt/edware/zones/landing/arrivals/   # this is where the file arrives.
        work: /opt/edware/zones/landing/work/           # this is the where the file are use for work. this should always be local for speed
        history: /opt/edware/zones/landing/history/     # this is where we store historical info. it may be an url for large file storages such as s3
    rsync:
      command: rsync -az --exclude "*.partial" --remove-source-files {remote_user}@{remote_host}:{remote_dir} {landing}
      schedule:
          cron:
            hour: '*/1'
      args:
        remote_user: udl2
        remote_host: lz.foo.com
        remote_dir: /sftp/opt/edware/staging/
        landing: /opt/edware/zones/landing/
    udl2_db_conn:
        url: postgresql://udl2:xON.yw27aw@rbtmqudl0.dmo.som.edwdc.net:5432/udl2
        db_schema: udl2
    target_db_conn:
        ca:
            url: postgresql://edware:xON.yw27aw@rbtmqudl0.dmo.som.edwdc.net:5432/edware
    prod_db_conn:
        ca:
            url: postgresql://edware:xON.yw27aw@dwpool0.dmo.som.edwdc.net:6432/edware
            db_schema: edware_es_1_6
    gpg_home: /opt/edware/keys
    edware_stats:
        db:
            schema_name: edware_es_1_6
            url: postgresql://edware:xON.yw27aw@dbpgdw0.dmo.som.edwdc.net:5432/edware_stats

development:
  '[app]':
    zones:                                              # zones for where the files are uploaded and processed. it may change to other mechanisms, but we uses local file system for the moment.
        landing: /opt/edware/zones/landing/             # this is for where the uploaded files are located, it may be an url in the long run to get data
        arrivals: /opt/edware/zones/landing/arrivals/   # this is where the file arrives.
        work: /opt/edware/zones/landing/work/           # this is the where the file are use for work. this should always be local for speed
        history: /opt/edware/zones/landing/history/     # this is where we store historical info. it may be an url for large file storages such as s3.
        datafiles: /opt/edware/zones/datafiles/         # this is for storing test sample data files
        tests: /opt/edware/zones/tests/                 # this is for running unit tests.
    rsync:
      command: rsync -az --exclude "*.partial" --remove-source-files {remote_user}@{remote_host}:{remote_dir} {landing}
      schedule:
          cron:
            hour: '*/1'
      args:
        remote_user: udl2
        remote_host: lz.foo.com
        remote_dir: /sftp/opt/edware/staging/
        landing: /opt/edware/zones/landing/
    udl2_db_conn:
        url: postgresql://udl2:udl2abc1234@localhost:5432/udl2
        db_schema: udl2
    edware_stats:
        db:
            schema_name: edware_stats
            url: postgresql://edware:edware2013@localhost:5432/edware_stats
    target_db_conn:
        cat:
            url: postgresql://edware:edware2013@localhost:5432/edware
        nc:
            url: postgresql://edware:edware2013@localhost:5432/edware
    prod_db_conn:
        cat:
            url: postgresql://edware:edware2013@localhost:5432/edware
            db_schema: edware_prod
        nc:
            url: postgresql://edware:edware2013@localhost:5432/edware
            db_schema: edware_prod
    gpg_home: ~/.gnupg
    sr_notification_max_attempts: 5
    sr_notification_retry_interval: 1
    sr_notification_timeout_interval: 1

jenksins_dev:
  '[app]':
    zones:                                              # zones for where the files are uploaded and processed. it may change to other mechanisms, but we uses local file system for the moment.
        landing: $WORKSPACE/opt/edware/zones/landing/             # this is for where the uploaded files are located, it may be an url in the long run to get data
        arrivals: $WORKSPACE/opt/edware/zones/landing/arrivals/   # this is where the file arrives.
        work: $WORKSPACE/opt/edware/zones/landing/work/           # this is the where the file are use for work. this should always be local for speed
        history: $WORKSPACE/opt/edware/zones/landing/history/     # this is where we store historical info. it may be an url for large file storages such as s3.
        datafiles: $WORKSPACE/opt/edware/zones/datafiles/         # this is for storing test sample data files
        tests: $WORKSPACE/opt/edware/zones/tests/                 # this is for running unit tests.
    rsync:
      command: rsync -az --exclude "*.partial" --remove-source-files {remote_user}@{remote_host}:{remote_dir} {landing}
      schedule:
          cron:
            hour: '*/1'
      args:
        remote_user: udl2
        remote_host: lz.foo.com
        remote_dir: /sftp/opt/edware/staging/
        landing: $WORKSPACE/opt/edware/zones/landing/
    udl2_db_conn:
        url: postgresql://udl2:udl2abc1234@localhost:5432/udl2
        db_schema: udl2
    edware_stats:
        db:
            schema_name: edware_stats
            url: postgresql://edware:edware2013@localhost:5432/edware_stats
    target_db_conn:
        ca:
            url: postgresql://edware:edware2013@localhost:5432/edware
    prod_db_conn:
        ca:
            url: postgresql://edware:edware2013@edwdbsrv4.poc.dum.edwdc.net:9999/edware
            db_schema: edware_sds_1_12_dog
    gpg_home: ~/.gnupg
